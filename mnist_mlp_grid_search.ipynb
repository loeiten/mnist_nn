{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we train an MLP to classify images from the MNIST database.\n",
    "\n",
    "### 1. Load MNIST Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "logging = False\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# use Keras to import pre-shuffled MNIST database\n",
    "# NOTE: Hold out one test set for final evaluation\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# NOTE: View in the jupyter console\n",
    "if logging:\n",
    "    session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rescale the Images by Dividing Every Pixel in Every Image by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale [0,255] --> [0,1]\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encode Categorical Integer Labels Using a One-Hot Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_ohe_train = np_utils.to_categorical(y_train, 10)\n",
    "y_ohe_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the Model Architecture Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "shape = x_train.shape[1:]\n",
    "\n",
    "def create_model(dropout_rate=0.2, hidden_layers=2, nodes=512, optimizer=\"rmsprop\"):\n",
    "    # From the tuorial https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "    print(\"...running create_model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=shape))\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(nodes, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print(\"Elapsed time: {}\\n\\n\\n\".format(str(timedelta(seconds=time.time() - start))))\n",
    "        return result\n",
    "              \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scan(scan_dict, train_scores, test_scores):\n",
    "    # From http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    ax.set_title(\"Validation curve\")\n",
    "    ax.set_xlabel(scan_dict[\"scan_param\"])\n",
    "    ax.set_ylabel(\"Score\")\n",
    "\n",
    "    ax.plot(scan_dict[\"param_range\"], train_scores_mean, label=\"Training score\")\n",
    "    ax.fill_between(scan_dict[\"param_range\"], train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    ax.plot(scan_dict[\"param_range\"], test_scores_mean, label=\"Cross-validation score\")\n",
    "    ax.fill_between(scan_dict[\"param_range\"], test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2)\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "@timer\n",
    "def perform_scan(x_train, y_train, scan_dict, model):\n",
    "    print(\"Scanning {}\".format(scan_dict[\"param_name\"]))\n",
    "    train_scores, test_scores =\\\n",
    "        validation_curve(model, \n",
    "                         x_train,\n",
    "                         y_train,\n",
    "                         param_name=scan_dict[\"param_name\"],\n",
    "                         param_range=scan_dict[\"param_range\"], \n",
    "                         cv=3, \n",
    "                         scoring=\"accuracy\")\n",
    "        \n",
    "    plot_scan(scan_dict, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate the validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# NOTE: 1/3 of the calculations are being done again every time here\n",
    "scan_dicts =[dict(param_name=\"dropout_rate\", scan_param=\"Dropout rate\", param_range=[0.0, 0.2, 0.4]),\n",
    "             dict(param_name=\"batch_size\", scan_param=\"Batch size\", param_range=[64, 128, 256]),\n",
    "             dict(param_name=\"epochs\", scan_param=\"Epochs\", param_range=[5, 10, 20]),\n",
    "             dict(param_name=\"hidden_layers\", scan_param=\"Hidden layers\", param_range=[1, 2, 3]),\n",
    "             dict(param_name=\"nodes\", scan_param=\"Nodes\", param_range=[256, 512, 1024]),\n",
    "             dict(param_name=\"optimizer\", scan_param=\"Optimizer\", param_range=[\"sgd\", \"rmsprop\", \"adadelta\"])]\n",
    "\n",
    "for scan_dict in scan_dicts:\n",
    "    perform_scan(x_train, y_ohe_train, scan_dict, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create the \"ultimate\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# We do so by combining the maximas in the created curves\n",
    "# NOTE: This is not very scientific\n",
    "\n",
    "model = create_model(dropout_rate=0.4, hidden_layers=2, nodes=512, optimizer=\"rmsprop\")\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "_ = model.fit(x_train, y_ohe_train, batch_size=64, epochs=20,\n",
    "              validation_split=0.2, callbacks=[checkpointer],\n",
    "              verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Load the Model with the Best Classification Accuracy on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights that yielded the best validation accuracy\n",
    "model.load_weights('mnist.model.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Calculate the Classification Accuracy on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test accuracy\n",
    "score = model.evaluate(x_test, y_ohe_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: {:.4f} %'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
